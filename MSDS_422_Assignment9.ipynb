{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weswest/MSDS422/blob/main/MSDS_422_Assignment9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y99TL9_LBxvv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwYRrG76u_hZ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCvI3RyDImNH"
      },
      "source": [
        "# 0 Project Overview\n",
        "\n",
        "This workbook focuses on the Disaster Tweets...\n",
        "\n",
        "https://www.kaggle.com/hritikchaturvedi/disaster-prediction-roberta-large/notebook?scriptVersionId=87180937"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ED-WZfOClff"
      },
      "source": [
        "# Workbook Structure\n",
        "\n",
        "TKTKTK\n",
        "\n",
        "## Considerations for analysis vs EDA\n",
        "\n",
        "TKTKTK\n",
        "\n",
        "\n",
        "\n",
        "## Overall layout\n",
        "\n",
        "TKTKTK\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOIvtnTDJYU9"
      },
      "source": [
        "# 0 Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bCljoEdqMxf"
      },
      "source": [
        "## 0.1 Setup - Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PW8FVBTm-u7",
        "outputId": "1f89b256-3582-42c2-f383-92d9192f3ead"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yi80XQ-7iiq2"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import RobertaTokenizer, TFRobertaModel, RobertaConfig \n",
        "from tensorflow.keras.layers import Input, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.initializers import TruncatedNormal\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "import pathlib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import io\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ObqSF_B6CduZ"
      },
      "outputs": [],
      "source": [
        "#from kerastuner.tuners import RandomSearch\n",
        "\n",
        "def set_seed(seed=422):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2d4GoxCxJBD"
      },
      "source": [
        "## 0.2 Setup - Operating Environment\n",
        "This code allows the Colab notebook to access my Google Drive files. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX_oH29qmsvu",
        "outputId": "0cc68852-d15f-4985-e40b-2d9a7a382b90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import os\n",
        "try:\n",
        "  os.chdir(\"drive/My Drive/MSDS/422/NLPTweets\")\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p3XUXKuJqWS"
      },
      "source": [
        "## 0.3 Setup - Read in Data\n",
        "Note: the Kaggle dataset already splits the housing data into \"train\" and \"test\" sets.  This assignment allows us to ignore the test set for now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KoowiScgicsF"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('Data/train.csv')\n",
        "train_df.name = 'Training Set'\n",
        "test_df = pd.read_csv('Data/test.csv')\n",
        "test_df.name = 'Test Set'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAJdYhNMXJ3J",
        "outputId": "e53896c0-368c-4325-b5b7-9dc3bb91f601"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Training Set we have 7613 observations, 5 variables: 2 numeric and 3 categorical\n",
            "In Test Set we have 3263 observations, 4 variables: 1 numeric and 3 categorical\n"
          ]
        }
      ],
      "source": [
        "dfs = [train_df, test_df]\n",
        "\n",
        "for df in dfs:\n",
        "  obs = df.shape[0]\n",
        "  tot = df.shape[1]\n",
        "  numeric = df.select_dtypes(include=np.number).shape[1]\n",
        "  categorical = df.select_dtypes(exclude=np.number).shape[1]\n",
        "  print('In {} we have {} observations, {} variables: {} numeric and {} categorical'.format(df.name, obs, tot, numeric, categorical))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qqtgG1hqiDU1",
        "outputId": "217bd1e8-124b-4f11-b956-738c6c0d9452"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a3c0f361-1ac9-440f-a00e-066314e6fc9c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3c0f361-1ac9-440f-a00e-066314e6fc9c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a3c0f361-1ac9-440f-a00e-066314e6fc9c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a3c0f361-1ac9-440f-a00e-066314e6fc9c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
              "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
              "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
              "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
              "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
              "\n",
              "   target  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XnObwYMoiQvS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rux8esOdLaL6"
      },
      "source": [
        "## 0.4 Set up functions for later reference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6WoTiHRXhZS"
      },
      "source": [
        "\n",
        "# 1 EDA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRah0kgviZvk",
        "outputId": "da1f73e4-b0ac-4c39-ab27-5a91c9e592a6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id             0\n",
              "keyword       61\n",
              "location    2533\n",
              "text           0\n",
              "target         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With that many null values in location, is our DV balanced?\n",
        "\n",
        "Answer: looks like the distribution is pretty consistent, with or without location"
      ],
      "metadata": {
        "id": "CnKQL5wvif6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[train_df.location.notnull()].target.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTJaqQsVibAR",
        "outputId": "886f8dd0-4df0-4159-d9f0-42b241573442"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2884\n",
              "1    2196\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[train_df.location.isnull()].target.value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7dMmUVjisfW",
        "outputId": "33380897-77e5-4726-dc95-79546c9889e1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1458\n",
              "1    1075\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Where are the locations?\n",
        "\n",
        "Answer: Some strong concentrations, but it descends into noise pretty fast.\n",
        "\n",
        "Good justification for deleting location entirely"
      ],
      "metadata": {
        "id": "RLRZTZrQi3yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[train_df.location.notnull()].location.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwGfEQs_i5ry",
        "outputId": "acbfa682-154e-4fff-aec6-bb3428e32973"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "USA                    104\n",
              "New York                71\n",
              "United States           50\n",
              "London                  45\n",
              "Canada                  29\n",
              "                      ... \n",
              "Montr√å¬©al, Qu√å¬©bec       1\n",
              "Montreal                 1\n",
              "√å√èT: 6.4682,3.18287      1\n",
              "Live4Heed??              1\n",
              "Lincoln                  1\n",
              "Name: location, Length: 3341, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3WZABE9eX-8"
      },
      "source": [
        "# 2. Prepare the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Drop Location"
      ],
      "metadata": {
        "id": "yhUKvYankLud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.drop(\"location\", axis = 1, inplace = True)\n",
        "test_df.drop(\"location\", axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "Wf-DWowhkNbw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Clean up the tweets"
      ],
      "metadata": {
        "id": "9pKIRg9ykp2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove urls\n",
        "\n",
        "def remove_URL(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'',text)\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(lambda x : remove_URL(x))\n",
        "test_df['text'] = test_df['text'].apply(lambda x : remove_URL(x))"
      ],
      "metadata": {
        "id": "KhxuGBEckr6v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove html\n",
        "\n",
        "example = \"\"\"<div>\n",
        "<h1>Real or Fake</h1>\n",
        "<p>Kaggle </p>\n",
        "<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n",
        "</div>\"\"\"\n",
        "\n",
        "def remove_html(text):\n",
        "    html=re.compile(r'<.*?>')\n",
        "    return html.sub(r'',text)\n",
        "    \n",
        "print(remove_html(example))\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(lambda x : remove_html(x))\n",
        "test_df['text'] = test_df['text'].apply(lambda x : remove_html(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOahMbXplCx_",
        "outputId": "56ef0c1c-76e9-4f62-d62c-684723642c24"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Real or Fake\n",
            "Kaggle \n",
            "getting started\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove emoji\n",
        "\n",
        "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "remove_emoji(\"Omg another Earthquake üòîüòî\")\n",
        "\n",
        "train_df['text'] = train_df['text'].apply(lambda x: remove_emoji(x))\n",
        "test_df['text'] = test_df['text'].apply(lambda x: remove_emoji(x))"
      ],
      "metadata": {
        "id": "948KiqBllP-D"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get rid of other issues\n",
        "\n",
        "def cleaner(tweet):\n",
        "  # Acronyms and miswritten words\n",
        "  tweet = re.sub(r\"Typhoon-Devastated\", \"typhoon devastated\", tweet)\n",
        "  tweet = re.sub(r\"TyphoonDevastated\", \"typhoon devastated\", tweet)\n",
        "  tweet = re.sub(r\"typhoondevastated\", \"typhoon devastated\", tweet)\n",
        "  tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight\", tweet)\n",
        "  tweet = re.sub(r\"MH\", \"Malaysia Airlines Flight\", tweet)\n",
        "  tweet = re.sub(r\"mh370\", \"Malaysia Airlines Flight\", tweet)\n",
        "  tweet = re.sub(r\"year-old\", \"years old\", tweet)\n",
        "  tweet = re.sub(r\"yearold\", \"years old\", tweet)\n",
        "  tweet = re.sub(r\"yr old\", \"years old\", tweet)\n",
        "  tweet = re.sub(r\"PKK\", \"Kurdistan Workers Party\", tweet)\n",
        "  tweet = re.sub(r\"MP\", \"madhya pradesh\", tweet)\n",
        "  tweet = re.sub(r\"rly\", \"railway\", tweet)\n",
        "  tweet = re.sub(r\"CDT\", \"Central Daylight Time\", tweet)\n",
        "  tweet = re.sub(r\"sensorsenso\", \"sensor senso\", tweet)\n",
        "  tweet = re.sub(r\"pm\", \"\", tweet)\n",
        "  tweet = re.sub(r\"PM\", \"\", tweet)\n",
        "  tweet = re.sub(r\"nan\", \" \", tweet)\n",
        "  tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n",
        "  tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n",
        "  tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n",
        "  tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n",
        "  tweet = re.sub(r\"prebreak\", \"pre break\", tweet)\n",
        "  tweet = re.sub(r\"nowplaying\", \"now playing\", tweet)\n",
        "  tweet = re.sub(r\"RT\", \"retweet\", tweet)\n",
        "  tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n",
        "  tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n",
        "  tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n",
        "  tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n",
        "  tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n",
        "  tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n",
        "  tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n",
        "  tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n",
        "  tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n",
        "  tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n",
        "\n",
        "  # Special characters\n",
        "  tweet = re.sub(r\"%20\", \" \", tweet)\n",
        "  tweet = re.sub(r\"%\", \" \", tweet)\n",
        "  tweet = re.sub(r\"@\", \" \", tweet)\n",
        "  tweet = re.sub(r\"#\", \" \", tweet)\n",
        "  tweet = re.sub(r\"'\", \" \", tweet)\n",
        "  tweet = re.sub(r\"\\x89√ª_\", \" \", tweet)\n",
        "  tweet = re.sub(r\"\\x89√ª√≤\", \" \", tweet)\n",
        "  tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n",
        "  tweet = re.sub(r\"re\\x89√ª_\", \" \", tweet)\n",
        "  tweet = re.sub(r\"\\x89√ª\", \" \", tweet)\n",
        "  tweet = re.sub(r\"\\x89√õ\", \" \", tweet)\n",
        "  tweet = re.sub(r\"re\\x89√õ\", \"re \", tweet)\n",
        "  tweet = re.sub(r\"re\\x89√ª\", \"re \", tweet)\n",
        "  tweet = re.sub(r\"\\x89√ª¬™\", \"'\", tweet)\n",
        "  tweet = re.sub(r\"\\x89√ª\", \" \", tweet)\n",
        "  tweet = re.sub(r\"\\x89√ª√≤\", \" \", tweet)\n",
        "  tweet = re.sub(r\"\\x89√õ_\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\x89√õ√í\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\x89√õ√ì\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\x89√õ√èWhen\", \"When\", tweet)\n",
        "  tweet = re.sub(r\"\\x89√õ√è\", \"\", tweet)\n",
        "  tweet = re.sub(r\"China\\x89√õ¬™s\", \"China's\", tweet)\n",
        "  tweet = re.sub(r\"let\\x89√õ¬™s\", \"let's\", tweet)\n",
        "  tweet = re.sub(r\"\\x89√õ√∑\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\x89√õ¬™\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\x89√õ\\x9d\", \"\", tweet)\n",
        "  tweet = re.sub(r\"√•_\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\x89√õ¬¢\", \"\", tweet)\n",
        "  tweet = re.sub(r\"\\x89√õ¬¢√•√ä\", \"\", tweet)\n",
        "  tweet = re.sub(r\"from√•√äwounds\", \"from wounds\", tweet)\n",
        "  tweet = re.sub(r\"√•√ä\", \"\", tweet)\n",
        "  tweet = re.sub(r\"√•√à\", \"\", tweet)\n",
        "  tweet = re.sub(r\"Jap√å_n\", \"Japan\", tweet)    \n",
        "  tweet = re.sub(r\"√å¬©\", \"e\", tweet)\n",
        "  tweet = re.sub(r\"√•¬®\", \"\", tweet)\n",
        "  tweet = re.sub(r\"Suru√å¬§\", \"Suruc\", tweet)\n",
        "  tweet = re.sub(r\"√•√á\", \"\", tweet)\n",
        "  tweet = re.sub(r\"√•¬£3million\", \"3 million\", tweet)\n",
        "  tweet = re.sub(r\"√•√Ä\", \"\", tweet)\n",
        "\n",
        "  # Contractions\n",
        "  tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
        "  tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
        "  tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
        "  tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
        "  tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
        "  tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
        "  tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
        "  tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
        "  tweet = re.sub(r\"don\\x89√õ¬™t\", \"do not\", tweet)\n",
        "  tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
        "  tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
        "  tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
        "  tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
        "  tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
        "  tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
        "  tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
        "  tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "  tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
        "  tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
        "  tweet = re.sub(r\"Im\", \"I am\", tweet)\n",
        "  tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
        "  tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
        "  tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
        "  tweet = re.sub(r\"I\\x89√õ¬™m\", \"I am\", tweet)\n",
        "  tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
        "  tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
        "  tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
        "  tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
        "  tweet = re.sub(r\"you\\x89√õ¬™ve\", \"you have\", tweet)\n",
        "  tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
        "  tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
        "  tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
        "  tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
        "  tweet = re.sub(r\"it\\x89√õ¬™s\", \"it is\", tweet)\n",
        "  tweet = re.sub(r\"doesn\\x89√õ¬™t\", \"does not\", tweet)\n",
        "  tweet = re.sub(r\"It\\x89√õ¬™s\", \"It is\", tweet)\n",
        "  tweet = re.sub(r\"Here\\x89√õ¬™s\", \"Here is\", tweet)\n",
        "  tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
        "  tweet = re.sub(r\"I\\x89√õ¬™ve\", \"I have\", tweet)\n",
        "  tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
        "  tweet = re.sub(r\"can\\x89√õ¬™t\", \"cannot\", tweet)\n",
        "  tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
        "  tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
        "  tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
        "  tweet = re.sub(r\"wouldn\\x89√õ¬™t\", \"would not\", tweet)\n",
        "  tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
        "  tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
        "  tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
        "  tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
        "  tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
        "  tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
        "  tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
        "  tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
        "  tweet = re.sub(r\"That\\x89√õ¬™s\", \"That is\", tweet)\n",
        "  tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
        "  tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "  tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
        "  tweet = re.sub(r\"You\\x89√õ¬™re\", \"You are\", tweet)\n",
        "  tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
        "  tweet = re.sub(r\"Don\\x89√õ¬™t\", \"Do not\", tweet)\n",
        "  tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
        "  tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "  tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
        "  tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
        "  tweet = re.sub(r\"Can\\x89√õ¬™t\", \"Cannot\", tweet)\n",
        "  tweet = re.sub(r\"you\\x89√õ¬™ll\", \"you will\", tweet)\n",
        "  tweet = re.sub(r\"I\\x89√õ¬™d\", \"I would\", tweet)\n",
        "  tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
        "  tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
        "  tweet = re.sub(r\"can't\", \"can not\", tweet)\n",
        "  tweet = re.sub(r\"cant\", \"can not\", tweet)\n",
        "  tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
        "  tweet = re.sub(r\"dont\", \"do not\", tweet)\n",
        "  tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
        "  tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
        "  tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
        "  tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
        "  tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
        "  tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
        "  tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
        "  tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
        "  tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
        "  tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
        "  tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
        "  tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
        "  tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
        "  tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
        "  tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
        "  tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
        "  tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
        "  tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
        "  tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
        "  tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
        "  tweet = re.sub(r\"don√•¬´t\", \"do not\", tweet)\n",
        "\n",
        "  return tweet\n",
        "train_df['text'] = train_df['text'].apply(lambda s : cleaner(s))\n",
        "test_df['text'] = test_df['text'].apply(lambda s : cleaner(s))"
      ],
      "metadata": {
        "id": "0BzpEMdrli-L"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punctuation\n",
        "\n",
        "def remove_punct(text):\n",
        "    table=str.maketrans('','',string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "example=\"I am a #king\"\n",
        "print(remove_punct(example))\n",
        "\n",
        "train_df['text']= train_df['text'].apply(lambda x : remove_punct(x))\n",
        "test_df['text']= test_df['text'].apply(lambda x : remove_punct(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbvcOt-9lrrr",
        "outputId": "b91f406c-b2ba-42e3-fe77-949cdbd64a9c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am a king\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove multiple spaces\n",
        "\n",
        "train_df['text'] = train_df['text'].str.replace('   ', ' ')\n",
        "train_df['text'] = train_df['text'].str.replace('     ', ' ')\n",
        "train_df['text'] = train_df['text'].str.replace('\\xa0 \\xa0 \\xa0', ' ')\n",
        "train_df['text'] = train_df['text'].str.replace('  ', ' ')\n",
        "train_df['text'] = train_df['text'].str.replace('‚Äî', ' ')\n",
        "train_df['text'] = train_df['text'].str.replace('‚Äì', ' ')\n",
        "\n",
        "test_df['text'] = test_df['text'].str.replace('   ', ' ')\n",
        "test_df['text'] = test_df['text'].str.replace('     ', ' ')\n",
        "test_df['text'] = test_df['text'].str.replace('\\xa0 \\xa0 \\xa0', ' ')\n",
        "test_df['text'] = test_df['text'].str.replace('  ', ' ')\n",
        "test_df['text'] = test_df['text'].str.replace('‚Äî', ' ')\n",
        "test_df['text'] = test_df['text'].str.replace('‚Äì', ' ')"
      ],
      "metadata": {
        "id": "x5r5gj43l-SX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Prepare the data for modeling"
      ],
      "metadata": {
        "id": "X1QQacommrWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select required columns\n",
        "data = train_df[['text', 'target']]\n",
        "\n",
        "# Set your model output as categorical and save in new label col\n",
        "data['target_label'] = pd.Categorical(train_df['target'])\n",
        "\n",
        "# Transform your output to numeric\n",
        "data['target'] = data['target_label'].cat.codes"
      ],
      "metadata": {
        "id": "jHXtgeKYmtbY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Build Models"
      ],
      "metadata": {
        "id": "JtQ2pLNlnI_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Load Model Transformer (RoBERTa)"
      ],
      "metadata": {
        "id": "KLmFmdW5nKX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### --------- Setup Roberta ---------- ###\n",
        "\n",
        "model_name = 'roberta-base'\n",
        "\n",
        "# Max length of tokens\n",
        "max_length = 45\n",
        "\n",
        "# Load transformers config and set output_hidden_states to False\n",
        "config = RobertaConfig.from_pretrained(model_name)\n",
        "config.output_hidden_states = False\n",
        "\n",
        "# Load Roberta tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n",
        "\n",
        "# Load the Roberta model\n",
        "transformer_roberta_model = TFRobertaModel.from_pretrained(model_name, config = config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOOlg2O7nM8t",
        "outputId": "17d01af8-a94b-4112-d38f-ff05a4bd4550"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Add more layers"
      ],
      "metadata": {
        "id": "bfmhQi5IoCiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### ------- Build the model ------- ###\n",
        "\n",
        "# Load the MainLayer\n",
        "roberta = transformer_roberta_model.layers[0]\n",
        "\n",
        "# Build your model input\n",
        "input_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
        "inputs = {'input_ids': input_ids}\n",
        "\n",
        "# Load the Transformers RoBERTa model as a layer in a Keras model\n",
        "roberta_model = roberta(inputs)[1]\n",
        "dropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\n",
        "pooled_output = dropout(roberta_model, training=False)\n",
        "\n",
        "dropout_1 = Dropout(config.hidden_dropout_prob, name='pooled_output_1')\n",
        "pooled_output_1 = dropout(pooled_output, training=False)\n",
        "\n",
        "# Then build your model output\n",
        "targets = Dense(units=len(data.target_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='target')(pooled_output_1)\n",
        "outputs = {'target': targets}\n",
        "\n",
        "# And combine it all in a model object\n",
        "# Note: building 3 models since there will be three iterations of testing\n",
        "model1 = Model(inputs=inputs, outputs=outputs, name='RoBERTa_Binary_Classifier1')\n",
        "model2 = Model(inputs=inputs, outputs=outputs, name='RoBERTa_Binary_Classifier2')\n",
        "model3 = Model(inputs=inputs, outputs=outputs, name='RoBERTa_Binary_Classifier3')\n",
        "\n"
      ],
      "metadata": {
        "id": "RBMJjtVQoG2S"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the model\n",
        "model1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyovVd9mkLdq",
        "outputId": "d6647b34-a8c1-4a65-ad16-72b6bd8bab7a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"RoBERTa_Binary_Classifier1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 45)]         0           []                               \n",
            "                                                                                                  \n",
            " roberta (TFRobertaMainLayer)   TFBaseModelOutputWi  124645632   ['input_ids[0][0]']              \n",
            "                                thPoolingAndCrossAt                                               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 45,                                                \n",
            "                                768),                                                             \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " pooled_output (Dropout)        (None, 768)          0           ['roberta[0][1]',                \n",
            "                                                                  'pooled_output[0][0]']          \n",
            "                                                                                                  \n",
            " target (Dense)                 (None, 2)            1538        ['pooled_output[1][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 124,647,170\n",
            "Trainable params: 124,647,170\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the model\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGVjBDIXkLDu",
        "outputId": "2e03c63c-ec74-4ba6-aa69-fc9a3e8311eb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"RoBERTa_Binary_Classifier2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 45)]         0           []                               \n",
            "                                                                                                  \n",
            " roberta (TFRobertaMainLayer)   TFBaseModelOutputWi  124645632   ['input_ids[0][0]']              \n",
            "                                thPoolingAndCrossAt                                               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 45,                                                \n",
            "                                768),                                                             \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " pooled_output (Dropout)        (None, 768)          0           ['roberta[0][1]',                \n",
            "                                                                  'pooled_output[0][0]']          \n",
            "                                                                                                  \n",
            " target (Dense)                 (None, 2)            1538        ['pooled_output[1][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 124,647,170\n",
            "Trainable params: 124,647,170\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the model\n",
        "model3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-nXodmGkHxU",
        "outputId": "ae6e42fd-42d9-4484-b4cc-444aecd2b8a3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"RoBERTa_Binary_Classifier3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 45)]         0           []                               \n",
            "                                                                                                  \n",
            " roberta (TFRobertaMainLayer)   TFBaseModelOutputWi  124645632   ['input_ids[0][0]']              \n",
            "                                thPoolingAndCrossAt                                               \n",
            "                                tentions(last_hidde                                               \n",
            "                                n_state=(None, 45,                                                \n",
            "                                768),                                                             \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=Non                                               \n",
            "                                e, cross_attentions                                               \n",
            "                                =None)                                                            \n",
            "                                                                                                  \n",
            " pooled_output (Dropout)        (None, 768)          0           ['roberta[0][1]',                \n",
            "                                                                  'pooled_output[0][0]']          \n",
            "                                                                                                  \n",
            " target (Dense)                 (None, 2)            1538        ['pooled_output[1][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 124,647,170\n",
            "Trainable params: 124,647,170\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Train the model"
      ],
      "metadata": {
        "id": "up9PCc_doV6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.1 Train Model 1 - Base Model"
      ],
      "metadata": {
        "id": "2XF-n85Ek9-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### ------- Train the model ------- ###\n",
        "\n",
        "optimizer = Adam(learning_rate=6e-05,epsilon=1e-08,decay=0.01,clipnorm=1.0)\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"Wk9Model1.h5\", save_best_only=True)\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "\n",
        "# Set loss and metrics\n",
        "loss = {'target': CategoricalCrossentropy(from_logits = True)}\n",
        "\n",
        "# Compile the model\n",
        "model1.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n",
        "\n",
        "# Ready output data for the model\n",
        "y_target = to_categorical(data['target'])\n",
        "\n",
        "# Tokenize the input (takes some time)\n",
        "x_train = tokenizer(\n",
        "            text=data['text'].to_list(),\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=True, \n",
        "            return_tensors='tf',\n",
        "            return_token_type_ids = False,\n",
        "            return_attention_mask = True,\n",
        "            verbose = True)\n",
        "\n",
        "# Fit the model\n",
        "history1 = model1.fit(\n",
        "    x={'input_ids': x_train['input_ids']},\n",
        "    y={'target': y_target},\n",
        "    validation_split=0.25,\n",
        "    batch_size=64,\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
        "    verbose=1)\n",
        "\n",
        "model1.save('Wk9Model1')   # Commented out so that a run doesn't overwrite the model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ey4gD9zhoZao",
        "outputId": "fa50c921-b40b-4a76-8630-4e6e2fd89d4c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "90/90 [==============================] - 97s 855ms/step - loss: 0.6824 - accuracy: 0.5738 - val_loss: 0.6926 - val_accuracy: 0.5494\n",
            "Epoch 2/50\n",
            "90/90 [==============================] - 59s 658ms/step - loss: 0.6824 - accuracy: 0.5773 - val_loss: 0.6886 - val_accuracy: 0.5494\n",
            "Epoch 3/50\n",
            "90/90 [==============================] - 58s 642ms/step - loss: 0.6665 - accuracy: 0.5985 - val_loss: 0.5884 - val_accuracy: 0.7258\n",
            "Epoch 4/50\n",
            "90/90 [==============================] - 57s 641ms/step - loss: 0.5183 - accuracy: 0.7630 - val_loss: 0.4330 - val_accuracy: 0.8246\n",
            "Epoch 5/50\n",
            "90/90 [==============================] - 42s 472ms/step - loss: 0.4439 - accuracy: 0.8092 - val_loss: 0.4976 - val_accuracy: 0.8041\n",
            "Epoch 6/50\n",
            "90/90 [==============================] - 57s 632ms/step - loss: 0.4038 - accuracy: 0.8290 - val_loss: 0.4039 - val_accuracy: 0.8309\n",
            "Epoch 7/50\n",
            "90/90 [==============================] - 42s 471ms/step - loss: 0.3840 - accuracy: 0.8394 - val_loss: 0.4223 - val_accuracy: 0.8256\n",
            "Epoch 8/50\n",
            "90/90 [==============================] - 42s 468ms/step - loss: 0.3574 - accuracy: 0.8508 - val_loss: 0.4691 - val_accuracy: 0.8078\n",
            "Epoch 9/50\n",
            "90/90 [==============================] - 42s 468ms/step - loss: 0.3398 - accuracy: 0.8600 - val_loss: 0.4071 - val_accuracy: 0.8319\n",
            "Epoch 10/50\n",
            "90/90 [==============================] - 42s 468ms/step - loss: 0.3278 - accuracy: 0.8707 - val_loss: 0.4189 - val_accuracy: 0.8340\n",
            "Epoch 11/50\n",
            "90/90 [==============================] - 42s 468ms/step - loss: 0.3184 - accuracy: 0.8746 - val_loss: 0.4856 - val_accuracy: 0.8204\n",
            "Epoch 12/50\n",
            "90/90 [==============================] - 42s 467ms/step - loss: 0.2999 - accuracy: 0.8812 - val_loss: 0.4273 - val_accuracy: 0.8361\n",
            "Epoch 13/50\n",
            "90/90 [==============================] - 42s 467ms/step - loss: 0.2922 - accuracy: 0.8858 - val_loss: 0.4602 - val_accuracy: 0.8288\n",
            "Epoch 14/50\n",
            "90/90 [==============================] - 42s 468ms/step - loss: 0.2689 - accuracy: 0.8951 - val_loss: 0.5103 - val_accuracy: 0.8220\n",
            "Epoch 15/50\n",
            "90/90 [==============================] - 42s 468ms/step - loss: 0.2617 - accuracy: 0.8988 - val_loss: 0.5122 - val_accuracy: 0.8319\n",
            "Epoch 16/50\n",
            "90/90 [==============================] - 42s 470ms/step - loss: 0.2549 - accuracy: 0.9037 - val_loss: 0.5103 - val_accuracy: 0.8199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses, embeddings_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: Wk9Model1/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: Wk9Model1/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = tokenizer(\n",
        "          text=test_df['text'].to_list(),\n",
        "          add_special_tokens=True,\n",
        "          max_length=max_length,\n",
        "          truncation=True,\n",
        "          padding=True, \n",
        "          return_tensors='tf',\n",
        "          return_token_type_ids = False,\n",
        "          return_attention_mask = True,\n",
        "          verbose = True)\n",
        "\n",
        "label_predicted1 = model1.predict(x={'input_ids': x_test['input_ids']},)\n",
        "label_pred_max1=[np.argmax(i) for i in label_predicted1['target']]\n",
        "output1 = pd.DataFrame({'id': test_df.id, 'target': label_pred_max1})\n",
        "output1.to_csv('Wk9Model1Take2.csv', index=False)"
      ],
      "metadata": {
        "id": "QhwcqZDY2Blt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.2 Train Model 2 - Decayed learning rate"
      ],
      "metadata": {
        "id": "VSv8T2m3lMBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### ------- Train the model ------- ###\n",
        "\n",
        "optimizer = Adam(learning_rate=6e-05,epsilon=1e-08,decay=0.01,clipnorm=1.0)\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"Wk9Model2.h5\", save_best_only=True)\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "lr_reduction_cb = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience = 3, verbose = 1, factor = 0.3)\n",
        "\n",
        "\n",
        "# Set loss and metrics\n",
        "loss = {'target': CategoricalCrossentropy(from_logits = True)}\n",
        "\n",
        "# Compile the model\n",
        "model2.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n",
        "\n",
        "# Ready output data for the model\n",
        "y_target = to_categorical(data['target'])\n",
        "\n",
        "# Tokenize the input (takes some time)\n",
        "x_train = tokenizer(\n",
        "            text=data['text'].to_list(),\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=True, \n",
        "            return_tensors='tf',\n",
        "            return_token_type_ids = False,\n",
        "            return_attention_mask = True,\n",
        "            verbose = True)\n",
        "\n",
        "# Fit the model\n",
        "history2 = model2.fit(\n",
        "    x={'input_ids': x_train['input_ids']},\n",
        "    y={'target': y_target},\n",
        "    validation_split=0.25,\n",
        "    batch_size=64,\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb, lr_reduction_cb],\n",
        "    verbose=1)\n",
        "\n",
        "model2.save('Wk9Model2')   # Commented out so that a run doesn't overwrite the model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd60c9c-8ee6-4582-a8a8-2c9673ab7f28",
        "id": "GBFQa6iWlMBJ"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "90/90 [==============================] - 101s 797ms/step - loss: 0.4476 - accuracy: 0.8208 - val_loss: 0.4392 - val_accuracy: 0.8277 - lr: 6.0000e-05\n",
            "Epoch 2/50\n",
            "90/90 [==============================] - 58s 651ms/step - loss: 0.3933 - accuracy: 0.8387 - val_loss: 0.4297 - val_accuracy: 0.8340 - lr: 6.0000e-05\n",
            "Epoch 3/50\n",
            "90/90 [==============================] - 43s 474ms/step - loss: 0.3641 - accuracy: 0.8506 - val_loss: 0.4416 - val_accuracy: 0.8241 - lr: 6.0000e-05\n",
            "Epoch 4/50\n",
            "90/90 [==============================] - 42s 468ms/step - loss: 0.3378 - accuracy: 0.8674 - val_loss: 0.4570 - val_accuracy: 0.8314 - lr: 6.0000e-05\n",
            "Epoch 5/50\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.3202 - accuracy: 0.8769\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.7999999545281754e-05.\n",
            "90/90 [==============================] - 42s 468ms/step - loss: 0.3202 - accuracy: 0.8769 - val_loss: 0.5425 - val_accuracy: 0.7988 - lr: 6.0000e-05\n",
            "Epoch 6/50\n",
            "90/90 [==============================] - 42s 467ms/step - loss: 0.2985 - accuracy: 0.8844 - val_loss: 0.4540 - val_accuracy: 0.8246 - lr: 1.8000e-05\n",
            "Epoch 7/50\n",
            "90/90 [==============================] - 42s 468ms/step - loss: 0.2979 - accuracy: 0.8891 - val_loss: 0.4521 - val_accuracy: 0.8325 - lr: 1.8000e-05\n",
            "Epoch 8/50\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2944 - accuracy: 0.8898\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 5.399999645305797e-06.\n",
            "90/90 [==============================] - 42s 468ms/step - loss: 0.2944 - accuracy: 0.8898 - val_loss: 0.4498 - val_accuracy: 0.8335 - lr: 1.8000e-05\n",
            "Epoch 9/50\n",
            "90/90 [==============================] - 42s 468ms/step - loss: 0.2876 - accuracy: 0.8923 - val_loss: 0.4652 - val_accuracy: 0.8298 - lr: 5.4000e-06\n",
            "Epoch 10/50\n",
            "90/90 [==============================] - 42s 469ms/step - loss: 0.2916 - accuracy: 0.8905 - val_loss: 0.4728 - val_accuracy: 0.8262 - lr: 5.4000e-06\n",
            "Epoch 11/50\n",
            "90/90 [==============================] - ETA: 0s - loss: 0.2887 - accuracy: 0.8921\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 1.6199999208765803e-06.\n",
            "90/90 [==============================] - 42s 469ms/step - loss: 0.2887 - accuracy: 0.8921 - val_loss: 0.4657 - val_accuracy: 0.8262 - lr: 5.4000e-06\n",
            "Epoch 12/50\n",
            "90/90 [==============================] - 42s 470ms/step - loss: 0.2854 - accuracy: 0.8949 - val_loss: 0.4715 - val_accuracy: 0.8256 - lr: 1.6200e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses, embeddings_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: Wk9Model2/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: Wk9Model2/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = tokenizer(\n",
        "          text=test_df['text'].to_list(),\n",
        "          add_special_tokens=True,\n",
        "          max_length=max_length,\n",
        "          truncation=True,\n",
        "          padding=True, \n",
        "          return_tensors='tf',\n",
        "          return_token_type_ids = False,\n",
        "          return_attention_mask = True,\n",
        "          verbose = True)\n",
        "\n",
        "label_predicted2 = model2.predict(x={'input_ids': x_test['input_ids']},)\n",
        "label_pred_max2=[np.argmax(i) for i in label_predicted2['target']]\n",
        "output2 = pd.DataFrame({'id': test_df.id, 'target': label_pred_max1})\n",
        "output2.to_csv('Wk9Model2Take2.csv', index=False)"
      ],
      "metadata": {
        "id": "kdESZN-j2QoW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.3 Train Model 3 - Different Optimizer"
      ],
      "metadata": {
        "id": "J27e3Cgalvwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### ------- Train the model ------- ###\n",
        "\n",
        "optimizer = SGD(learning_rate=6e-05, momentum = 0, clipnorm=1.0)\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"Wk9Model3.h5\", save_best_only=True)\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "\n",
        "# Set loss and metrics\n",
        "loss = {'target': CategoricalCrossentropy(from_logits = True)}\n",
        "\n",
        "# Compile the model\n",
        "model3.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n",
        "\n",
        "# Ready output data for the model\n",
        "y_target = to_categorical(data['target'])\n",
        "\n",
        "# Tokenize the input (takes some time)\n",
        "x_train = tokenizer(\n",
        "            text=data['text'].to_list(),\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=True, \n",
        "            return_tensors='tf',\n",
        "            return_token_type_ids = False,\n",
        "            return_attention_mask = True,\n",
        "            verbose = True)\n",
        "\n",
        "# Fit the model\n",
        "history3 = model3.fit(\n",
        "    x={'input_ids': x_train['input_ids']},\n",
        "    y={'target': y_target},\n",
        "    validation_split=0.25,\n",
        "    batch_size=64,\n",
        "    epochs=50,\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
        "    verbose=1)\n",
        "\n",
        "model3.save('Wk9Model3')   # Commented out so that a run doesn't overwrite the model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f75562-b83d-476e-ab9b-f34e862e4f68",
        "id": "vXOIDtTflvwc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "90/90 [==============================] - 79s 567ms/step - loss: 0.3336 - accuracy: 0.8685 - val_loss: 0.4309 - val_accuracy: 0.8361\n",
            "Epoch 2/50\n",
            "90/90 [==============================] - 44s 495ms/step - loss: 0.3345 - accuracy: 0.8655 - val_loss: 0.4278 - val_accuracy: 0.8361\n",
            "Epoch 3/50\n",
            "90/90 [==============================] - 39s 428ms/step - loss: 0.3496 - accuracy: 0.8599 - val_loss: 0.4283 - val_accuracy: 0.8356\n",
            "Epoch 4/50\n",
            "90/90 [==============================] - 38s 427ms/step - loss: 0.3460 - accuracy: 0.8623 - val_loss: 0.4427 - val_accuracy: 0.8340\n",
            "Epoch 5/50\n",
            "90/90 [==============================] - 38s 427ms/step - loss: 0.3412 - accuracy: 0.8642 - val_loss: 0.4326 - val_accuracy: 0.8361\n",
            "Epoch 6/50\n",
            "90/90 [==============================] - 38s 427ms/step - loss: 0.3384 - accuracy: 0.8641 - val_loss: 0.4331 - val_accuracy: 0.8377\n",
            "Epoch 7/50\n",
            "90/90 [==============================] - 38s 428ms/step - loss: 0.3464 - accuracy: 0.8604 - val_loss: 0.4356 - val_accuracy: 0.8372\n",
            "Epoch 8/50\n",
            "90/90 [==============================] - 38s 427ms/step - loss: 0.3498 - accuracy: 0.8607 - val_loss: 0.4330 - val_accuracy: 0.8367\n",
            "Epoch 9/50\n",
            "90/90 [==============================] - 44s 485ms/step - loss: 0.3445 - accuracy: 0.8600 - val_loss: 0.4241 - val_accuracy: 0.8393\n",
            "Epoch 10/50\n",
            "90/90 [==============================] - 38s 428ms/step - loss: 0.3510 - accuracy: 0.8553 - val_loss: 0.4268 - val_accuracy: 0.8388\n",
            "Epoch 11/50\n",
            "90/90 [==============================] - 43s 483ms/step - loss: 0.3483 - accuracy: 0.8588 - val_loss: 0.4237 - val_accuracy: 0.8409\n",
            "Epoch 12/50\n",
            "90/90 [==============================] - 39s 428ms/step - loss: 0.3439 - accuracy: 0.8620 - val_loss: 0.4306 - val_accuracy: 0.8382\n",
            "Epoch 13/50\n",
            "90/90 [==============================] - 38s 427ms/step - loss: 0.3496 - accuracy: 0.8597 - val_loss: 0.4330 - val_accuracy: 0.8346\n",
            "Epoch 14/50\n",
            "90/90 [==============================] - 38s 427ms/step - loss: 0.3477 - accuracy: 0.8567 - val_loss: 0.4294 - val_accuracy: 0.8382\n",
            "Epoch 15/50\n",
            "90/90 [==============================] - 38s 427ms/step - loss: 0.3440 - accuracy: 0.8604 - val_loss: 0.4308 - val_accuracy: 0.8377\n",
            "Epoch 16/50\n",
            "90/90 [==============================] - 38s 427ms/step - loss: 0.3470 - accuracy: 0.8599 - val_loss: 0.4289 - val_accuracy: 0.8388\n",
            "Epoch 17/50\n",
            "90/90 [==============================] - 38s 427ms/step - loss: 0.3474 - accuracy: 0.8585 - val_loss: 0.4391 - val_accuracy: 0.8361\n",
            "Epoch 18/50\n",
            "90/90 [==============================] - 38s 427ms/step - loss: 0.3448 - accuracy: 0.8614 - val_loss: 0.4314 - val_accuracy: 0.8356\n",
            "Epoch 19/50\n",
            "90/90 [==============================] - 38s 427ms/step - loss: 0.3407 - accuracy: 0.8607 - val_loss: 0.4417 - val_accuracy: 0.8346\n",
            "Epoch 20/50\n",
            "90/90 [==============================] - 38s 427ms/step - loss: 0.3485 - accuracy: 0.8628 - val_loss: 0.4362 - val_accuracy: 0.8340\n",
            "Epoch 21/50\n",
            "90/90 [==============================] - 39s 429ms/step - loss: 0.3420 - accuracy: 0.8595 - val_loss: 0.4357 - val_accuracy: 0.8356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses, embeddings_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: Wk9Model3/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: Wk9Model3/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = tokenizer(\n",
        "          text=test_df['text'].to_list(),\n",
        "          add_special_tokens=True,\n",
        "          max_length=max_length,\n",
        "          truncation=True,\n",
        "          padding=True, \n",
        "          return_tensors='tf',\n",
        "          return_token_type_ids = False,\n",
        "          return_attention_mask = True,\n",
        "          verbose = True)\n",
        "\n",
        "label_predicted3 = model3.predict(x={'input_ids': x_test['input_ids']},)\n",
        "label_pred_max3=[np.argmax(i) for i in label_predicted3['target']]\n",
        "output3 = pd.DataFrame({'id': test_df.id, 'target': label_pred_max1})\n",
        "output3.to_csv('Wk9Mode31Take2.csv', index=False)"
      ],
      "metadata": {
        "id": "2PUMxKI82ewY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Predict"
      ],
      "metadata": {
        "id": "68QzqH6korN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i41zPhc22DeQ"
      },
      "execution_count": 30,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "MSDS-422 Assignment9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJGL7TWp8XjlipzTd0BDkJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}